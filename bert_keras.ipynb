{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv(\"data\\\\data.csv\")\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.groupby(['Feeling']).size().plot.bar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def remove_url(text):\n",
    "    return re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"Remove punctuation\"\"\"\n",
    "    translator = str.maketrans(\"\",\"\",string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "def remove_stopwords(text):\n",
    "    filtered_words = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "def remove_numbers(text):\n",
    "    filtered_words = [i for i in text.split() if not i.isdigit()]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "stop_words = [\"ed\",\"rt\",\"tweet\",\"tweeted\"]\n",
    "def remove_freq(text):\n",
    "    filtered_words = [word.lower() for word in text.split() if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "def remove_non_latin(text):\n",
    "    return re.sub(r'[^\\x00-\\x7F]+','', text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the functions to the dataframe\n",
    "tweets_df['Tweets'] = tweets_df['Tweets'].apply(remove_url)\n",
    "tweets_df['Tweets'] = tweets_df['Tweets'].apply(remove_punctuation)\n",
    "tweets_df['Tweets'] = tweets_df['Tweets'].apply(remove_stopwords)\n",
    "tweets_df['Tweets'] = tweets_df['Tweets'].apply(remove_numbers)\n",
    "tweets_df['Tweets'] = tweets_df['Tweets'].apply(remove_freq)\n",
    "tweets_df['Tweets'] = tweets_df['Tweets'].apply(remove_non_latin)\n",
    "tweets_df['Tweets'] = tweets_df['Tweets'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use English stemmer.\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "tweets_df['tokenized'] = tweets_df.apply(lambda row: nltk.word_tokenize(row['Tweets']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only english words from tokenized column\n",
    "words = set(nltk.corpus.words.words())\n",
    "tweets_df['tokenized'] = tweets_df['tokenized'].apply(lambda x: [item for item in x if item in words])\n",
    "\n",
    "tweets_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use English stemmer.\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "tweets_df['stemmed'] = tweets_df['tokenized'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "tweets_df['stemmed'] = tweets_df.apply(lambda row: TreebankWordDetokenizer().detokenize(row['stemmed']), axis=1)\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets_df['stemmed']\n",
    "y = pd.get_dummies(tweets_df['Feeling']).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the full data 80:20 into training:validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, random_state=101)\n",
    "\n",
    "# split training data 87.5:12.5 into training:testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, train_size=0.875, random_state=101)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "preprocessed_text = bert_preprocess(text_input)\n",
    "outputs = bert_encoder(preprocessed_text)\n",
    "\n",
    "l = tf.keras.layers.Dropout(0.1, name=\"dropout\")(outputs['pooled_output'])\n",
    "l = tf.keras.layers.Dense(6, activation='sigmoid', name=\"output\")(l)\n",
    "\n",
    "model = tf.keras.Model(inputs=[text_input], outputs = [l])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall')\n",
    "]\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    " loss='binary_crossentropy',\n",
    " metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "446091c97b97ed08451f92ccbf5462419c292e2401edec9be09d06e2a7cee579"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
