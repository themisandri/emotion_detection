{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aEyF3x4g7Yz2"
   },
   "outputs": [],
   "source": [
    "pip install -q tf-models-official==2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "afNOrIMW7Yz-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from official import nlp\n",
    "from official.nlp import bert\n",
    "from official.nlp import optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xB9Mq6kT7Y0A"
   },
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv(\"data\\\\data.csv\")\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gt1uOocy7Y0B",
    "outputId": "6d8f1402-ce15-4c67-f1e2-20774e402599"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def remove_url(text):\n",
    "    return re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"Remove punctuation\"\"\"\n",
    "    translator = str.maketrans(\"\",\"\",string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "def remove_stopwords(text):\n",
    "    filtered_words = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "def remove_numbers(text):\n",
    "    filtered_words = [i for i in text.split() if not i.isdigit()]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "stop_words = [\"ed\",\"rt\",\"tweet\",\"tweeted\"]\n",
    "def remove_freq(text):\n",
    "    filtered_words = [word.lower() for word in text.split() if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "def remove_non_latin(text):\n",
    "    return re.sub(r'[^\\x00-\\x7F]+','', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2EjG10Kw7Y0D"
   },
   "outputs": [],
   "source": [
    "tweets_df['Tweets'] = tweets_df['Tweets'].apply(remove_url)\n",
    "tweets_df['Tweets'] = tweets_df['Tweets'].apply(remove_punctuation)\n",
    "tweets_df['Tweets'] = tweets_df['Tweets'].apply(remove_stopwords)\n",
    "tweets_df['Tweets'] = tweets_df['Tweets'].apply(remove_numbers)\n",
    "tweets_df['Tweets'] = tweets_df['Tweets'].apply(remove_freq)\n",
    "tweets_df['Tweets'] = tweets_df['Tweets'].apply(remove_non_latin)\n",
    "tweets_df['Tweets'] = tweets_df['Tweets'].str.replace('\\d+', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use English stemmer.\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "tweets_df['tokenized'] = tweets_df.apply(lambda row: nltk.word_tokenize(row['Tweets']), axis=1)\n",
    "tweets_df['stemmed'] = tweets_df['tokenized'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "tweets_df['stemmed'] = tweets_df.apply(lambda row: TreebankWordDetokenizer().detokenize(row['stemmed']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5ZWhGOT7Y0F"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count unique words to calculate number of unique words for tokenization\n",
    "def counter_word(text_col):\n",
    "    count = Counter()\n",
    "    for text in text_col.values:\n",
    "        for word in text.split():\n",
    "            count[word] += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "counter = counter_word(tweets_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F8PyBOE37Y0G",
    "outputId": "c0dbee93-632c-431d-b83c-e1b143394cd3"
   },
   "outputs": [],
   "source": [
    "len(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8S2k-3b77Y0H",
    "outputId": "ea02eaf7-4001-43a8-fcbf-272a6aa5b9eb"
   },
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYTLngyA7Y0J"
   },
   "outputs": [],
   "source": [
    "num_unique_words = len(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hGWl0-CC7Y0K",
    "outputId": "8c33087a-2cf5-41d7-cd10-7b889603e02c"
   },
   "outputs": [],
   "source": [
    "counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emDiErUB7Y0L"
   },
   "outputs": [],
   "source": [
    "X = tweets_df['stemmed']\n",
    "y = pd.get_dummies(tweets_df['Feeling']).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGldsURQ7Y0M"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the full data 80:20 into training:validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, random_state=101)\n",
    "\n",
    "# split training data 87.5:12.5 into training:testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, train_size=0.875, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bWh3TZa_7Y0N"
   },
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# vectorize text by turning each text into a sequence of integers\n",
    "tokenizer = Tokenizer(num_words=num_unique_words)\n",
    "tokenizer.fit_on_texts(X_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HgGAiT2K7Y0N",
    "outputId": "7608a585-21ff-4944-ad3a-9e7eafa4cb0c"
   },
   "outputs": [],
   "source": [
    "index_of_words = tokenizer.word_index\n",
    "\n",
    "# vacab size is number of unique words + reserved 0 index for padding\n",
    "vocab_size = len(index_of_words) + 1\n",
    "\n",
    "print('Number of unique words: {}'.format(len(index_of_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBg5Uq7k7Y0O"
   },
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_valid = tokenizer.texts_to_sequences(X_valid)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qxInqwDG7Y0P",
    "outputId": "f4ccc22d-83ac-41bb-c35f-149aa127d48b"
   },
   "outputs": [],
   "source": [
    "print(X_train[10:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OIKDIhLX7Y0Q",
    "outputId": "c12a4486-e931-4d86-8e7c-47b25e82cc86"
   },
   "outputs": [],
   "source": [
    "# Pad the sequences to have the same length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Max number of words in a sequence\n",
    "max_length = 500\n",
    "max_len = -1\n",
    "for ele in tweets_df.text: \n",
    "    if len(ele) > max_len: \n",
    "        max_len = len(ele) \n",
    "        res = ele \n",
    "print(max_len)\n",
    "\n",
    "\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "X_valid = pad_sequences(X_valid, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "X_test = pad_sequences(X_test, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "X_train.shape,  X_valid.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PpzUk34G7Y0R"
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    with open(filepath,encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mn7JLuYa7Y0S",
    "outputId": "1c825674-d5ed-4009-b0b5-09bdd6d0c4c9"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "fname = 'embeddings/wiki-news-300d-1M.vec'\n",
    "\n",
    "if not os.path.isfile(fname):\n",
    "    print('Downloading word vectors...')\n",
    "    urllib.request.urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip',\n",
    "                              'wiki-news-300d-1M.vec.zip')\n",
    "    print('Unzipping...')\n",
    "    with zipfile.ZipFile('wiki-news-300d-1M.vec.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('embeddings')\n",
    "    print('done.')\n",
    "    \n",
    "    os.remove('wiki-news-300d-1M.vec.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LUAL3hXf7Y0T",
    "outputId": "7a5d5d19-c893-40f8-dde6-992208a95fd0"
   },
   "outputs": [],
   "source": [
    "# Number of dimensions for word embedding\n",
    "embed_num_dims = 300\n",
    "\n",
    "embedd_matrix = create_embedding_matrix(fname, index_of_words, embed_num_dims)\n",
    "embedd_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rKV_afQ07Y0U",
    "outputId": "8c48c90a-fb67-4800-bfeb-e4ac06b5ecbc"
   },
   "outputs": [],
   "source": [
    "# Inspect unseen words\n",
    "new_words = 0\n",
    "\n",
    "for word in index_of_words:\n",
    "    entry = embedd_matrix[index_of_words[word]]\n",
    "    if all(v == 0 for v in entry):\n",
    "        new_words = new_words + 1\n",
    "\n",
    "print('Words found in wiki vocab: ' + str(len(index_of_words) - new_words))\n",
    "print('New words found: ' + str(new_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3D1UeKF84pA"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ch6TIzZ7Y0V",
    "outputId": "46365e88-9a16-4475-f879-6898d0ee0c72"
   },
   "outputs": [],
   "source": [
    "# Convolution\n",
    "kernel_size = 3\n",
    "filters = 32\n",
    "\n",
    "# Embedding layer before the actaul CNN \n",
    "embedd_layer = tf.keras.layers.Embedding(vocab_size,\n",
    "                         embed_num_dims,\n",
    "                         input_length = 500,\n",
    "                         weights = [embedd_matrix],\n",
    "                         trainable=False)\n",
    "\n",
    "model2 = tf.keras.models.Sequential()\n",
    "model2.add(embedd_layer)\n",
    "model2.add(tf.keras.layers.Conv1D(filters, kernel_size, activation='relu'))\n",
    "model2.add(tf.keras.layers.BatchNormalization())\n",
    "model2.add(tf.keras.layers.Conv1D(filters, kernel_size, activation='relu'))\n",
    "model2.add(tf.keras.layers.BatchNormalization())\n",
    "model2.add(tf.keras.layers.Conv1D(filters, kernel_size, activation='relu'))\n",
    "model2.add(tf.keras.layers.BatchNormalization())\n",
    "model2.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model2.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model2.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model2.add(tf.keras.layers.Dense(7, activation='softmax'))\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "model2.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy',f1_m,precision_m, recall_m])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hist = model2.fit(X_train, y_train, \n",
    "                 batch_size=batch_size,\n",
    "                 epochs=epochs,\n",
    "                 validation_data=(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GzUkIT_t7Y0Z",
    "outputId": "cca3ae12-dea4-4a71-9680-c314316bcbe5"
   },
   "outputs": [],
   "source": [
    "loss, accuracy, f1_score, precision, recall = model2.evaluate(X_test, y_test, verbose=0)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HROW0V3XBrpz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "bn2OldOfBr7f",
    "outputId": "90f11c60-a996-4d39-8f2a-447ee36d8e12"
   },
   "outputs": [],
   "source": [
    "#model architecture\n",
    "\n",
    "# Embedding layer before the actaul BLSTM \n",
    "embedd_layer = tf.keras.layers.Embedding(vocab_size,\n",
    "                         embed_num_dims,\n",
    "                         input_length = 500,\n",
    "                         weights = [embedd_matrix],\n",
    "                         trainable=False)\n",
    "\n",
    "\n",
    "model3 = tf.keras.models.Sequential()\n",
    "model3.add(embedd_layer)\n",
    "model3.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=128,\n",
    "                              dropout=0.2,\n",
    "                              recurrent_dropout=0.2, return_sequences=True)))\n",
    "model3.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=128,\n",
    "                              dropout=0.2,\n",
    "                              recurrent_dropout=0.2, return_sequences=True)))\n",
    "model3.add(tf.keras.layers.Dense(128,activation = tf.nn.relu))                             \n",
    "model3.add(tf.keras.layers.Dense(7, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',f1_m,precision_m, recall_m])\n",
    "hist = model3.fit(X_train, y_train, epochs=10, validation_data=(X_valid,y_valid),batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mj-7AhFVHC0B"
   },
   "outputs": [],
   "source": [
    "loss, accuracy, f1_score, precision, recall = model3.evaluate(X_test, y_test, verbose=0)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgCwg2Pw7Y0a"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "message = ['I am really happy that you won']\n",
    "\n",
    "seq = tokenizer.texts_to_sequences(message)\n",
    "padded = pad_sequences(seq, maxlen=max_length)\n",
    "\n",
    "start_time = time.time()\n",
    "pred = model2.predict(padded)\n",
    "\n",
    "print('Message: ' + str(message))\n",
    "print('predicted: {} ({:.2f} seconds)'.format(class_names[np.argmax(pred)], (time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "797wRnFI7Y0b"
   },
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dXUZrkke7Y0b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "fianl_word2vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5ce1cdf7e1cfecddf081d38adb70c5b89bbbc939f7b33263749e4b31988ad6b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
